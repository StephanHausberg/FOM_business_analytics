{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc124b10-7e87-4946-adb7-fcc364bff728",
   "metadata": {},
   "source": [
    "# Ausrüster AG Use Case\n",
    "\n",
    "## Python Notebook: FOM - Area of Application - Business Analytics\n",
    "\n",
    "Author: Dr. Stephan Hausberg, Winter semester 2024\n",
    "\n",
    "Learning objectives:\n",
    "\n",
    "- Learn how to load packages\n",
    "- Learn how to read data from flat files (xlsx, csv)\n",
    "- Learn basic record linkage\n",
    "- Learn how to make basic descriptive analytics\n",
    "\n",
    "1. Loading packages\n",
    "\n",
    "Basic packages for tabular data and numeric operations are pandas and numpy. Pandas stands for panel data. Numpy for Numeric python. A convention is to load them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af726c7-6c48-4e9c-b098-b66a7b8bd626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bbe795-32ae-444d-bd98-e29e16de22e5",
   "metadata": {},
   "source": [
    "If that does not work in your python environment, install these via pip install pandas or in your anaconda environment. To show all functions a package, you could use the dir command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036cc422-8650-49fe-a0da-79f31241d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(pd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed00b89-331e-4da7-9cb4-e99d917994cf",
   "metadata": {},
   "source": [
    "By the way - these Jupyter Notebooks are a very good tool to develop code and documentation to a broader audience. Jupyter is a prject that stands for Julia - Python - R, since these are the languages it was created for.\n",
    "\n",
    "2. Read data from flat files\n",
    "\n",
    "Usually, there are different kinds of data. They are all stored in different technological flavors. Databases, flat files, cloud storages, parquet files, or JSON files. Let's start with parsing simple tabular data from csv files. CSV stands for comma seperated values, saying values are usually seperated by commata. Let's see what happens and how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d5f3e-aaeb-408f-8dc4-8ed1c4a19b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"ausruester_ag_01_customer_small.csv\" , sep = \";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e8b692-5a36-4ae1-b400-aba31a56d447",
   "metadata": {},
   "source": [
    "As we see here is that parsing without a specification on which seperator is chosen throws an error. It is necessary to know that in this case a semicolon is the used seperator here. To see all parameters and annotations that are usually stored in the docstring of a function, write help. Here this reads as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c7db5a-9607-4833-939a-918a1aacd26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d240f2e0-375b-42b8-b831-933734e8ebcb",
   "metadata": {},
   "source": [
    "Another usual case is parsing MS Excel files. Therefore, again, a pandas function exists.Sometimes it is useful to specify an engine like openpyxl, depending on the Excel-file-version your are using here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123f798f-467c-4796-abd9-f4ccb1220784",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel(\"ausruester_ag_01_customer_small.xlsx\", engine = \"openpyxl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b366a3ed-9ef4-4e3e-9ee1-0e3c77fd7c84",
   "metadata": {},
   "source": [
    "We store this last dataframe in a variable called 'df_customer_machines'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9990fc-3510-4da9-8f00-82c8cd48125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_machine = pd.read_excel(\"ausruester_ag_01_customer_small.xlsx\",\n",
    "                                    engine = \"openpyxl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8424e1d4-2e9f-4a30-95ea-5f4a5bf6ac5c",
   "metadata": {},
   "source": [
    "3. Data model and record linkage\n",
    "\n",
    "Everybodys hope is that there is an underlying data model. If there is a documentation about it, great. If not, you have to ask people who work with the data or try to figure it out yourself. In our case, we are lucky and find 3 different tables. \n",
    "\n",
    "- customer data with their current machine status\n",
    "- customer data ono how to approach them\n",
    "- customer locations where they produce\n",
    "We read in this data. They are all connected via the column called ‘Client_ID‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd44865-f351-4b75-ae1f-a77df5ba38c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_base = pd.read_excel(\"ausruester_ag_02_customer_base.xlsx\")\n",
    "\n",
    "df_customer_loc = pd.read_excel(\"ausruester_ag_03_locations.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff9df4d-1e49-4cb7-8287-08a4d19eb8cf",
   "metadata": {},
   "source": [
    "To get a brief overview of your tables, you could concatenate with the point-operator the corresponding methods like head() or tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5e2054-ab8b-4dbc-93ea-0c0fb6358d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_loc.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b513f13d-5f4d-4f03-bc47-c0e2fc58c103",
   "metadata": {},
   "source": [
    "To get a brief overview on how many row and columns we face, use the shape attribute. Also the value_counts method is a very easy standard method to get an overwiew or count for column values. Here, we perform this for the given locations and the Client_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faebf37f-12ff-43cb-9924-343f1c27e565",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_loc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10c600e-8a64-4aaa-9292-1d93ee09df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_loc.Client_ID.value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba2016c-4fac-478e-942c-e8558e9adf6d",
   "metadata": {},
   "source": [
    "There are all standard join or record linkages techniques which you might know from SQL included in the pandas package. That is, for example, left-join, right-join, inner-join, and outer-join. We would like to find out, which name of service performed is linked to which clients with their corresponding contact person. So, basically, we would like to join the customer base with a left join to our machine base but only the name of the serviec performed. Let's take a look at the data.\n",
    "\n",
    "The base table is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cf2cae-6ab2-4267-b0b7-416e08028bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab8de2-5ee0-4780-abe8-5e17d39366ed",
   "metadata": {},
   "source": [
    "The machine table is given by the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4833713d-38c4-46cd-ad5e-458d38177e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0891d795-a8f4-4dfd-a8a6-37dea44ed520",
   "metadata": {},
   "source": [
    "To only select the corresponding interesting tables, we simply put these in brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fbb329-5619-4680-a806-be48b55586ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_machine[['Client', 'Name of the service performed']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb8393f-9c59-45fa-9456-fa6ed39f499b",
   "metadata": {},
   "source": [
    "We realize that only the client names are given and the Client_ID column is missing in the machine-table. Let's try, if we can match anyways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3cb54b-c960-4023-bd99-1430aa271f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_machine.insert(0, \"Client_ID\", [1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd9a18c-efb3-4145-8a46-439ca6f11b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_base.merge(df_customer_machine[['Client', 'Name of the service performed']], on = 'Client', how ='left ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0feefc-8474-443d-9b62-8168325c5c41",
   "metadata": {},
   "source": [
    "This seems to throw an error. We could take a critical look and see that we could fix it with sorting and renaming the columns, but we take a different way by inserting the column \"Client_ID\" and figure out if this helps. By using the merge method we "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e0183c-a15f-4765-934e-b32d4ca0249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_machine.insert(0, \"Client_ID\", [1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37867ac1-a967-47ee-ba2b-b1d68019d77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_base.merge(df_customer_machine[['Client_ID', 'Name of the service performed']], on = 'Client_ID', how ='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ab87c-a1e6-4045-9785-604a57a11b74",
   "metadata": {},
   "source": [
    "There are many ways how you can merge, join and concanete data. This is usually the everyday groundwork of any data analyst. So, please feel free to check this out as many times as you can! Here, so many things can go wrong, for example, there are more than one relationship between two tables, saying a 1:n relation. In our case this be checked by taking a closer look at the table where locations are given. The unique ID here is the combination of Client_ID and Standort_ID. So, as simple merge-function would give you more than 6 records here, i.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf5dbe6-469a-4f0d-9ef0-4594b2dd9ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_base.merge(df_customer_loc, on = 'Client_ID', how = \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b24b7fb-8173-44c4-a96a-dab4d0cc6e00",
   "metadata": {},
   "source": [
    "Duplicates are the result of this operation. Therefore you should handle data transformation with much care within a given data model. A solution to this specific case could be the drop duplicates method. We assume that the first given location in the loc-table is the headquarter here. We have then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0080c6a-82dc-406a-924c-b6b0c9c16f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_base.merge(df_customer_loc.drop_duplicates(subset = 'Client_ID', keep = 'first'), on = 'Client_ID', how = \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41483ae8-74f0-4f0f-a8f2-d31c6e488907",
   "metadata": {},
   "source": [
    "4. Simple descriptive analytics\n",
    "\n",
    "Descriptive analytics often feel like giving an overview of mean, standard deviation or quantiles. A perfect method comes through pandas itself, i.e. describe(). Did you know that mean is a very good prediction model from where to start? Sure, it depends on so many more things that could influence values, but this predicts the mean, which is better than nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b2dba6-b82b-4fd5-a230-5c23787768d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_machine.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f684ab-bdc3-4c26-abb4-23c3f2be707e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "For sure, there are many ways to compute statistical measures in Python via suitable packages. Also it is possible to visualize them easily. At this point it is much more important to have a brief overview of where to start your exploration journey to not get lost in details.\n",
    "\n",
    "### SweetViz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7531fe0c-5d32-4ed7-9cf4-25ac3ca4d001",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sweetviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3c3f1f-e7dd-468e-81e0-d0908002d527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sweetviz as sv\n",
    "\n",
    "my_report = sv.analyze(df_customer_machine)\n",
    "my_report.show_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d2fc1f-7c6d-4e6e-a93f-f6abc32de5a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Summary Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85784c10-8103-4259-ae7d-1ef09c49cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install jupyter-summarytools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b43239-804b-44cd-9919-747715204f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarytools import dfSummary\n",
    "dfSummary(df_customer_machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72b87f7-48a9-4ca3-bda9-9704f53d1152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
