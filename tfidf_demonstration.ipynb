{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF: Term Frequency - Inverse Document Frequency\n",
    "\n",
    "## What is TF-IDF?\n",
    "\n",
    "TF-IDF is a numerical statistic that reflects how important a word is to a document in a collection of documents.\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "TF-IDF(term, document) = TF(term, document) √ó IDF(term)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **TF (Term Frequency)**: How often a term appears in a document\n",
    "- **IDF (Inverse Document Frequency)**: How rare/common a term is across all documents\n",
    "\n",
    "**Key Insight:** Words that appear frequently in one document but rarely in others get high TF-IDF scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Styling\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Sample Documents\n",
    "\n",
    "Let's use a simple corpus of documents about different topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus: 4 documents about different topics\n",
    "documents = [\n",
    "    \"The cat sat on the mat. The cat was happy.\",\n",
    "    \"The dog played in the garden. The dog loves to run.\",\n",
    "    \"Machine learning is a subset of artificial intelligence. Machine learning algorithms learn from data.\",\n",
    "    \"Python is a programming language. Python is popular for data science and machine learning.\"\n",
    "]\n",
    "\n",
    "doc_names = ['Doc 1: Cat', 'Doc 2: Dog', 'Doc 3: ML', 'Doc 4: Python']\n",
    "\n",
    "# Display documents\n",
    "print(\"üìö Our Document Corpus:\\n\")\n",
    "for i, (name, doc) in enumerate(zip(doc_names, documents), 1):\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  '{doc}'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Step 1: Term Frequency (TF)\n",
    "\n",
    "**Term Frequency** measures how often a term appears in a document.\n",
    "\n",
    "Formula:\n",
    "```\n",
    "TF(term, doc) = (Number of times term appears in doc) / (Total number of terms in doc)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Term Frequency for Document 1\n",
    "doc1 = documents[0].lower()\n",
    "words = doc1.split()\n",
    "\n",
    "# Count word occurrences\n",
    "word_counts = Counter(words)\n",
    "total_words = len(words)\n",
    "\n",
    "print(f\"Document 1: '{documents[0]}'\\n\")\n",
    "print(f\"Total words: {total_words}\")\n",
    "print(f\"\\nWord counts:\")\n",
    "for word, count in word_counts.most_common():\n",
    "    tf = count / total_words\n",
    "    print(f\"  '{word}': appears {count} times ‚Üí TF = {count}/{total_words} = {tf:.3f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Raw counts\n",
    "words_list = list(word_counts.keys())\n",
    "counts_list = list(word_counts.values())\n",
    "axes[0].barh(words_list, counts_list, color='steelblue', alpha=0.7)\n",
    "axes[0].set_xlabel('Count', fontsize=12)\n",
    "axes[0].set_title('Raw Word Counts (Document 1)', fontsize=14, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Term Frequency\n",
    "tf_list = [c / total_words for c in counts_list]\n",
    "axes[1].barh(words_list, tf_list, color='coral', alpha=0.7)\n",
    "axes[1].set_xlabel('Term Frequency', fontsize=12)\n",
    "axes[1].set_title('Term Frequency (Normalized)', fontsize=14, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì TF normalizes word counts by document length!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Step 2: Inverse Document Frequency (IDF)\n",
    "\n",
    "**IDF** measures how rare or common a term is across all documents.\n",
    "\n",
    "Formula:\n",
    "```\n",
    "IDF(term) = log(Total number of documents / Number of documents containing term)\n",
    "```\n",
    "\n",
    "**Key:** \n",
    "- Common words (appear in many documents) ‚Üí Low IDF\n",
    "- Rare words (appear in few documents) ‚Üí High IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IDF for all unique words\n",
    "from collections import defaultdict\n",
    "\n",
    "# Get all unique words and document frequency\n",
    "doc_frequency = defaultdict(int)\n",
    "all_words = set()\n",
    "\n",
    "for doc in documents:\n",
    "    words_in_doc = set(doc.lower().split())\n",
    "    all_words.update(words_in_doc)\n",
    "    for word in words_in_doc:\n",
    "        doc_frequency[word] += 1\n",
    "\n",
    "# Calculate IDF\n",
    "n_documents = len(documents)\n",
    "idf_scores = {}\n",
    "\n",
    "for word in all_words:\n",
    "    idf = np.log(n_documents / doc_frequency[word])\n",
    "    idf_scores[word] = idf\n",
    "\n",
    "# Sort by IDF score\n",
    "sorted_idf = sorted(idf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display results\n",
    "print(f\"Total documents: {n_documents}\\n\")\n",
    "print(\"IDF Scores (sorted by rarity):\\n\")\n",
    "print(f\"{'Word':<20} {'Appears in Docs':<20} {'IDF Score':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for word, idf in sorted_idf[:15]:  # Show top 15\n",
    "    doc_count = doc_frequency[word]\n",
    "    print(f\"{word:<20} {doc_count}/{n_documents} documents{' ':<8} {idf:.4f}\")\n",
    "\n",
    "# Visualize IDF scores\n",
    "top_words = [w for w, _ in sorted_idf[:12]]\n",
    "top_idfs = [idf_scores[w] for w in top_words]\n",
    "colors = ['red' if doc_frequency[w] == 1 else 'orange' if doc_frequency[w] == 2 else 'green' \n",
    "          for w in top_words]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(range(len(top_words)), top_idfs, color=colors, alpha=0.7)\n",
    "plt.xticks(range(len(top_words)), top_words, rotation=45, ha='right')\n",
    "plt.ylabel('IDF Score', fontsize=12)\n",
    "plt.title('Inverse Document Frequency (IDF) Scores', fontsize=14, fontweight='bold')\n",
    "plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='red', alpha=0.7, label='Appears in 1 doc (rare)'),\n",
    "    Patch(facecolor='orange', alpha=0.7, label='Appears in 2 docs'),\n",
    "    Patch(facecolor='green', alpha=0.7, label='Appears in 3+ docs (common)')\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc='upper right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Rare words get higher IDF scores!\")\n",
    "print(\"‚úì Common words (like 'the') get lower IDF scores!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Step 3: TF-IDF = TF √ó IDF\n",
    "\n",
    "Now we combine Term Frequency and Inverse Document Frequency:\n",
    "\n",
    "```\n",
    "TF-IDF(term, doc) = TF(term, doc) √ó IDF(term)\n",
    "```\n",
    "\n",
    "This gives us a score that is:\n",
    "- **High** when a term appears frequently in a document but rarely in others\n",
    "- **Low** when a term appears in many documents (common words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual TF-IDF calculation for Document 1\n",
    "doc1_words = documents[0].lower().split()\n",
    "doc1_word_counts = Counter(doc1_words)\n",
    "doc1_total_words = len(doc1_words)\n",
    "\n",
    "print(\"üìä TF-IDF Calculation for Document 1:\\n\")\n",
    "print(f\"{'Word':<15} {'TF':<10} {'√ó':<5} {'IDF':<10} {'=':<5} {'TF-IDF':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "doc1_tfidf = {}\n",
    "for word in set(doc1_words):\n",
    "    tf = doc1_word_counts[word] / doc1_total_words\n",
    "    idf = idf_scores[word]\n",
    "    tfidf = tf * idf\n",
    "    doc1_tfidf[word] = tfidf\n",
    "    print(f\"{word:<15} {tf:<10.4f} √ó {idf:<10.4f} = {tfidf:<10.4f}\")\n",
    "\n",
    "# Sort by TF-IDF score\n",
    "sorted_tfidf = sorted(doc1_tfidf.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nüéØ Words ranked by importance (TF-IDF) in Document 1:\")\n",
    "for i, (word, score) in enumerate(sorted_tfidf, 1):\n",
    "    print(f\"{i}. '{word}': {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. TF-IDF with scikit-learn\n",
    "\n",
    "In practice, we use scikit-learn's `TfidfVectorizer` for efficient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(lowercase=True, stop_words=None)\n",
    "\n",
    "# Fit and transform documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert to DataFrame for easier viewing\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),\n",
    "    columns=feature_names,\n",
    "    index=doc_names\n",
    ")\n",
    "\n",
    "print(\"üìä TF-IDF Matrix (all documents):\\n\")\n",
    "print(tfidf_df.round(3))\n",
    "print(f\"\\nShape: {tfidf_df.shape} (4 documents √ó {len(feature_names)} unique words)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Visualizing TF-IDF Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of TF-IDF scores\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Select top words by max TF-IDF score\n",
    "top_n = 15\n",
    "max_scores = tfidf_df.max(axis=0)\n",
    "top_words = max_scores.nlargest(top_n).index\n",
    "tfidf_subset = tfidf_df[top_words]\n",
    "\n",
    "sns.heatmap(tfidf_subset.T, annot=True, fmt='.3f', cmap='YlOrRd', \n",
    "            cbar_kws={'label': 'TF-IDF Score'}, linewidths=0.5)\n",
    "plt.title(f'TF-IDF Heatmap: Top {top_n} Important Words Across Documents', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Documents', fontsize=12)\n",
    "plt.ylabel('Words', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà Interpretation:\")\n",
    "print(\"  ‚Ä¢ Darker red = Higher TF-IDF score = More important/distinctive word\")\n",
    "print(\"  ‚Ä¢ Light yellow = Lower score = Common or less relevant word\")\n",
    "print(\"  ‚Ä¢ Notice how topic-specific words (cat, dog, machine, python) have high scores!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top words per document\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, doc_name in enumerate(doc_names):\n",
    "    # Get top 8 words for this document\n",
    "    doc_scores = tfidf_df.loc[doc_name].sort_values(ascending=False).head(8)\n",
    "    \n",
    "    # Filter out zero scores\n",
    "    doc_scores = doc_scores[doc_scores > 0]\n",
    "    \n",
    "    # Plot\n",
    "    colors = plt.cm.RdYlGn_r(doc_scores / doc_scores.max())\n",
    "    axes[idx].barh(range(len(doc_scores)), doc_scores.values, color=colors)\n",
    "    axes[idx].set_yticks(range(len(doc_scores)))\n",
    "    axes[idx].set_yticklabels(doc_scores.index)\n",
    "    axes[idx].set_xlabel('TF-IDF Score', fontsize=10)\n",
    "    axes[idx].set_title(f'{doc_name}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].invert_yaxis()\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Most Important Words per Document (TF-IDF)', \n",
    "             fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Comparison: With vs. Without Stop Words\n",
    "\n",
    "Stop words are common words (like \"the\", \"is\", \"a\") that usually don't carry much meaning. Let's see the effect of removing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF without stop words\n",
    "vectorizer_no_stop = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "tfidf_matrix_no_stop = vectorizer_no_stop.fit_transform(documents)\n",
    "\n",
    "feature_names_no_stop = vectorizer_no_stop.get_feature_names_out()\n",
    "tfidf_df_no_stop = pd.DataFrame(\n",
    "    tfidf_matrix_no_stop.toarray(),\n",
    "    columns=feature_names_no_stop,\n",
    "    index=doc_names\n",
    ")\n",
    "\n",
    "# Comparison\n",
    "print(\"üìä Comparison: With vs. Without Stop Words\\n\")\n",
    "print(f\"With stop words:    {len(feature_names)} unique words\")\n",
    "print(f\"Without stop words: {len(feature_names_no_stop)} unique words\")\n",
    "print(f\"Reduction:          {len(feature_names) - len(feature_names_no_stop)} words removed\\n\")\n",
    "\n",
    "# Show removed words (stop words)\n",
    "stop_words_removed = set(feature_names) - set(feature_names_no_stop)\n",
    "print(f\"Stop words removed: {sorted(stop_words_removed)}\")\n",
    "\n",
    "# Visualize comparison for Document 3 (ML document)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# With stop words\n",
    "doc3_with = tfidf_df.loc['Doc 3: ML'].sort_values(ascending=False).head(10)\n",
    "doc3_with = doc3_with[doc3_with > 0]\n",
    "axes[0].barh(range(len(doc3_with)), doc3_with.values, color='steelblue', alpha=0.7)\n",
    "axes[0].set_yticks(range(len(doc3_with)))\n",
    "axes[0].set_yticklabels(doc3_with.index)\n",
    "axes[0].set_xlabel('TF-IDF Score')\n",
    "axes[0].set_title('With Stop Words', fontsize=12, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Without stop words\n",
    "doc3_without = tfidf_df_no_stop.loc['Doc 3: ML'].sort_values(ascending=False).head(10)\n",
    "doc3_without = doc3_without[doc3_without > 0]\n",
    "axes[1].barh(range(len(doc3_without)), doc3_without.values, color='coral', alpha=0.7)\n",
    "axes[1].set_yticks(range(len(doc3_without)))\n",
    "axes[1].set_yticklabels(doc3_without.index)\n",
    "axes[1].set_xlabel('TF-IDF Score')\n",
    "axes[1].set_title('Without Stop Words', fontsize=12, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.suptitle('Top Words in ML Document (Doc 3)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Removing stop words helps focus on meaningful content words!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Real-World Application: Document Similarity\n",
    "\n",
    "TF-IDF is often used to compute document similarity. Similar documents will have similar TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate cosine similarity between all documents\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix_no_stop)\n",
    "\n",
    "# Create DataFrame\n",
    "similarity_df = pd.DataFrame(\n",
    "    similarity_matrix,\n",
    "    index=doc_names,\n",
    "    columns=doc_names\n",
    ")\n",
    "\n",
    "print(\"üìä Document Similarity Matrix (Cosine Similarity):\\n\")\n",
    "print(similarity_df.round(3))\n",
    "print(\"\\n1.0 = Identical documents, 0.0 = Completely different documents\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "mask = np.triu(np.ones_like(similarity_matrix, dtype=bool), k=1)\n",
    "sns.heatmap(similarity_df, annot=True, fmt='.3f', cmap='coolwarm', \n",
    "            center=0.5, vmin=0, vmax=1, mask=mask,\n",
    "            square=True, linewidths=1, cbar_kws={'label': 'Similarity'})\n",
    "plt.title('Document Similarity Based on TF-IDF', fontsize=14, fontweight='bold', pad=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Insights:\")\n",
    "print(\"  ‚Ä¢ Doc 3 (ML) and Doc 4 (Python) are most similar - both discuss tech topics\")\n",
    "print(\"  ‚Ä¢ Doc 1 (Cat) and Doc 2 (Dog) have some similarity - both about animals\")\n",
    "print(\"  ‚Ä¢ Animal docs and tech docs are quite different from each other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Summary: When to Use TF-IDF\n",
    "\n",
    "### ‚úÖ Use TF-IDF for:\n",
    "- **Text classification** - converting text to numerical features\n",
    "- **Information retrieval** - finding relevant documents\n",
    "- **Document clustering** - grouping similar documents\n",
    "- **Keyword extraction** - identifying important terms\n",
    "- **Document comparison** - measuring similarity\n",
    "\n",
    "### üîë Key Takeaways:\n",
    "1. **TF-IDF highlights distinctive words** - words that are frequent in a document but rare across the corpus\n",
    "2. **Common words get low scores** - words like \"the\", \"is\", \"a\" are downweighted\n",
    "3. **Better than raw counts** - considers both local (TF) and global (IDF) importance\n",
    "4. **Foundation for NLP** - widely used in search engines, recommendation systems, and text analysis\n",
    "\n",
    "### ‚ö†Ô∏è Limitations:\n",
    "- Ignores word order and context\n",
    "- Doesn't capture semantic meaning\n",
    "- For advanced tasks, consider: Word2Vec, BERT, or other embedding methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization: TF-IDF concept summary\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# TF example\n",
    "axes[0].bar(['Word A', 'Word B', 'Word C'], [5, 2, 1], color='skyblue', alpha=0.7)\n",
    "axes[0].set_title('Term Frequency (TF)\\nHow often in THIS document?', \n",
    "                  fontsize=11, fontweight='bold')\n",
    "axes[0].set_ylabel('Count in Document')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# IDF example\n",
    "axes[1].bar(['Word A\\n(appears in\\n3/4 docs)', \n",
    "             'Word B\\n(appears in\\n2/4 docs)', \n",
    "             'Word C\\n(appears in\\n1/4 docs)'], \n",
    "            [0.3, 0.7, 1.4], color='orange', alpha=0.7)\n",
    "axes[1].set_title('Inverse Document Frequency (IDF)\\nHow rare across ALL documents?', \n",
    "                  fontsize=11, fontweight='bold')\n",
    "axes[1].set_ylabel('IDF Score')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# TF-IDF example\n",
    "tfidf_values = [5*0.3, 2*0.7, 1*1.4]\n",
    "colors_final = ['lightcoral', 'gold', 'lightgreen']\n",
    "bars = axes[2].bar(['Word A', 'Word B', 'Word C'], tfidf_values, \n",
    "                   color=colors_final, alpha=0.7)\n",
    "axes[2].set_title('TF-IDF = TF √ó IDF\\nFinal importance score', \n",
    "                  fontsize=11, fontweight='bold')\n",
    "axes[2].set_ylabel('TF-IDF Score')\n",
    "axes[2].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, tfidf_values):\n",
    "    height = bar.get_height()\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Understanding TF-IDF: The Complete Picture', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Remember: TF-IDF = Term Frequency √ó Inverse Document Frequency\")\n",
    "print(\"   ‚Üí Highlights words that are important to a specific document!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
